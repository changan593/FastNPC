# FastNPC 测试与评估系统使用指南

## 📋 概述

FastNPC 测试与评估系统是一个完整的提示词质量管理框架，用于系统化地测试和评估LLM提示词的效果。

## 🎯 核心功能

### 1. 测试用例管理

**测试角色标记**
- 支持为角色和群聊标记为"测试用例"
- 测试角色在列表中会显示星标 ⭐
- 可以随时重置测试角色的状态（清空对话历史和记忆）

**测试用例类型**
- **单聊测试**：针对单个角色的对话能力测试
- **群聊测试**：针对群组的多角色互动测试
- **结构化生成测试**：测试从百科全文生成结构化角色信息的质量
- **简介生成测试**：测试角色简介生成的质量
- **短期记忆测试**：测试对话记忆凝练功能
- **长期记忆测试**：测试记忆整合功能
- **群聊中控测试**：测试群聊调度决策质量

### 2. 评估提示词管理

**7种专业评估器**
1. **结构化生成评估器**：评估完整性、准确性、一致性、结构性、可用性
2. **简介生成评估器**：评估吸引力、准确性、简洁性、完整性
3. **单聊对话评估器**：评估角色一致性、相关性、流畅性、深度、互动性
4. **群聊对话评估器**：评估角色区分度、互动质量、调度合理性、连贯性、氛围
5. **短期记忆凝练评估器**：评估信息保留度、压缩率、重点突出、可读性
6. **长期记忆整合评估器**：评估去重效果、归纳总结、重要性排序、结构组织
7. **群聊中控评估器**：评估选人准确性、理由合理性、置信度校准、公平性

**评估器特点**
- 基于LLM的自动化评估
- 标准化评分体系（0-50分或0-40分）
- JSON格式输出，易于解析和分析
- 支持版本管理和历史对比

### 3. 测试配置管理

每个测试用例可以配置：
- **LLM参数**：
  - `max_tokens`：最大生成token数
  - `temperature`：温度参数
  - `model`：使用的模型

- **记忆预算**：
  - `ctx_max_chat`：最大对话轮次
  - `ctx_max_stm`：最大短期记忆条数
  - `ctx_max_ltm`：最大长期记忆条数

- **群聊配置**：
  - `max_group_reply_rounds`：最大群聊回复轮数
  - `expected_rounds`：期望的对话轮次

## 🚀 快速开始

### 步骤1：初始化数据库

首先需要运行数据库迁移，添加必要的字段：

```bash
cd /home/changan/MyProject/FastNPC
python -m fastnpc.api.auth.db_init
```

### 步骤2：创建测试角色和群聊

在管理员账号下创建以下50个角色（已按群组分类）：

**政治局**：特朗普，普京，泽连斯基  
**诗词局**：李白，杜甫，苏轼，李商隐，李清照  
**神仙局**：哪吒，孙悟空，杨戬，观音菩萨  
**资本局**：马斯克，马云，马化腾，雷军，黄仁勋  
**三国局**：曹操，刘备，吕布，诸葛亮，周瑜，孙权，司马懿，貂蝉，小乔  
**明星局**：胡歌，马嘉祺，蔡徐坤，杨幂  
**名人局**：罗永浩，罗翔，贾国龙  
**巨人局**：艾伦·耶格尔，阿明·阿诺德，三笠·阿克曼，利威尔·阿克曼，莱纳·布朗  
**医疗局**：张伯礼，钟南山，李时珍，孙思邈  
**科学局**：爱因斯坦，牛顿，玛丽·居里，杨振宁  
**异人局**：张楚岚，冯宝宝，张之维，王也  

然后创建对应的11个群聊，将角色加入各自的群组。

### 步骤3：初始化评估提示词

运行评估提示词初始化脚本：

```bash
cd /home/changan/MyProject/FastNPC
python fastnpc/scripts/init_evaluation_prompts.py
```

这将创建7个专业评估器的初始版本。

### 步骤4：批量生成测试用例

运行测试用例生成脚本：

```bash
cd /home/changan/MyProject/FastNPC
python fastnpc/scripts/generate_test_cases.py
```

这个脚本将：
- 为每个角色生成4个单聊测试用例
- 为每个角色生成1个结构化生成测试用例
- 为每个群聊生成3个群聊测试用例
- 自动标记所有角色和群聊为测试用例

预计生成约：
- 50 × 4 = 200 个单聊测试用例
- 50 × 1 = 50 个结构化生成测试用例
- 11 × 3 = 33 个群聊测试用例
- **总计：约 283 个测试用例**

## 🎮 使用方法

### 在Web界面使用

#### 1. 访问测试管理

1. 登录管理员账号
2. 点击"管理员"按钮
3. 在管理员面板中点击"🎯 提示词与测试管理"

#### 2. 管理提示词

在"🎯 提示词"标签页：
- 左侧：选择提示词分类和子分类
- 中间：编辑提示词内容和描述
- 右侧：运行测试、查看测试结果

主要操作：
- **保存新版本**：修改后保存为新版本
- **激活版本**：将当前版本设为激活状态
- **版本历史**：查看和切换历史版本
- **复制**：基于当前版本创建副本
- **运行测试**：对当前提示词运行相关测试用例

#### 3. 管理评估器

在"⭐ 评估"标签页：
- 左侧：选择评估器类型
- 中间：编辑评估标准和评分体系
- 右侧：查看评估器信息和使用提示

关键功能：
- 修改评分维度和权重
- 调整评分标准（优秀/良好/合格的分数线）
- 更新评估提示词模板
- 管理评估器版本

#### 4. 管理测试用例

在"🧪 测试用例"标签页：
- 左侧上部：测试分类导航
- 左侧下部：当前分类的测试用例列表
- 中间：测试用例详情（描述、测试内容、期望行为、配置）
- 右侧：执行历史和结果展示

主要操作：
- **执行测试**：运行当前测试用例
- **重置状态**：清空测试角色/群聊的对话历史和记忆
- **查看历史**：查看过往执行记录
- **分析结果**：查看评分、反馈、LLM响应

### 通过API使用

#### 标记测试角色

```python
import requests

# 标记角色为测试用例
response = requests.post(
    'http://localhost:8000/admin/characters/{character_id}/mark-test-case',
    json={'is_test_case': True},
    cookies={'session': 'your-session-cookie'}
)

# 标记群聊为测试用例
response = requests.post(
    'http://localhost:8000/admin/groups/{group_id}/mark-test-case',
    json={'is_test_case': True},
    cookies={'session': 'your-session-cookie'}
)
```

#### 重置测试状态

```python
# 重置角色状态
response = requests.post(
    'http://localhost:8000/admin/test-cases/reset-character/{character_name}',
    cookies={'session': 'your-session-cookie'}
)

# 重置群聊状态
response = requests.post(
    'http://localhost:8000/admin/test-cases/reset-group/{group_id}',
    cookies={'session': 'your-session-cookie'}
)
```

#### 执行测试

```python
# 执行单个测试用例
response = requests.post(
    'http://localhost:8000/admin/test-cases/{test_case_id}/execute',
    cookies={'session': 'your-session-cookie'}
)

# 批量执行测试
response = requests.post(
    'http://localhost:8000/admin/test-cases/batch-execute',
    json={'test_case_ids': [1, 2, 3]},
    cookies={'session': 'your-session-cookie'}
)
```

#### 查询测试报告

```python
# 获取测试报告
response = requests.get(
    'http://localhost:8000/admin/test-reports',
    params={
        'test_case_id': 1,
        'limit': 10
    },
    cookies={'session': 'your-session-cookie'}
)

data = response.json()
print(f"通过率: {data['statistics']['pass_rate']}%")
print(f"平均分: {data['statistics']['average_score']}")
```

## 💡 最佳实践

### 1. 测试用例设计原则

- **针对性**：每个测试用例应该针对角色的特点设计专门的测试内容
- **多样性**：覆盖不同的对话场景和话题
- **可重复性**：确保测试结果的稳定性和可比较性
- **版本化**：测试内容也应该版本管理，便于对照实验

### 2. 评估器使用建议

- **一致性**：保持评估标准的稳定，避免频繁修改
- **客观性**：设计客观可量化的评估维度
- **全面性**：从多个角度评估，不要只关注单一指标
- **对比性**：使用相同的评估器对比不同版本的提示词

### 3. 测试流程建议

1. **基准测试**：首先对当前提示词进行全面测试，建立baseline
2. **修改提示词**：基于测试结果调整提示词
3. **A/B测试**：同时保留两个版本，对比效果
4. **记录分析**：详细记录每次修改和效果变化
5. **迭代优化**：基于数据驱动持续改进

### 4. 配置调优建议

- **记忆预算**：
  - 简单对话：ctx_max_chat=10, ctx_max_stm=3, ctx_max_ltm=5
  - 复杂对话：ctx_max_chat=20, ctx_max_stm=5, ctx_max_ltm=10
  - 专业深度对话：ctx_max_chat=30, ctx_max_stm=10, ctx_max_ltm=15

- **群聊轮次**：
  - 快节奏群聊：max_group_reply_rounds=3
  - 一般群聊：max_group_reply_rounds=5
  - 深度讨论：max_group_reply_rounds=8

- **LLM参数**：
  - 严肃对话：temperature=0.5-0.7
  - 创意对话：temperature=0.8-1.0
  - 结构化生成：temperature=0.3-0.5

## 📊 测试报告解读

### 评分体系

所有评估器都采用多维度评分：
- 每个维度 0-10 分
- 总分 = 各维度分数之和
- 根据总分判定是否通过（passed: true/false）

### 通过标准

**单聊/群聊对话评估器**（总分50分）：
- ≥40分：优秀 ✓
- 35-39分：良好 ✓
- 30-34分：合格 ✓
- <30分：不合格 ✗

**结构化生成评估器**（总分50分）：
- ≥40分：优秀 ✓
- 35-39分：良好 ✓
- 30-34分：合格 ✓
- <30分：不合格 ✗

**简介生成评估器**（总分40分）：
- ≥32分：优秀 ✓
- 28-31分：良好 ✓
- 24-27分：合格 ✓
- <24分：不合格 ✗

**记忆相关评估器**（总分40分）：
- ≥32分：优秀 ✓
- 28-31分：良好 ✓
- 24-27分：合格 ✓
- <24分：不合格 ✗

### 关键指标

执行历史中记录的关键数据：
- **passed**：是否通过测试
- **score**：总得分
- **duration_ms**：执行耗时
- **evaluation_feedback**：详细反馈
- **evaluation_result**：完整评估结果（JSON）
- **llm_response**：LLM的原始响应

## 🔧 故障排查

### 常见问题

**Q: 测试用例生成失败？**
A: 确保所有角色和群聊都已创建，并且角色名称完全匹配。

**Q: 评估器不工作？**
A: 检查评估提示词是否已初始化，并且至少有一个版本处于激活状态。

**Q: 执行测试超时？**
A: 检查OpenRouter API配置，增加timeout设置，或者减少测试复杂度。

**Q: 评分异常？**
A: 可能是评估提示词的评分标准不清晰，建议重新编辑评估器模板。

### 调试技巧

1. **查看执行日志**：在 `logs/` 目录下查看详细日志
2. **单步测试**：先对单个测试用例执行，排查问题
3. **检查API响应**：使用浏览器开发者工具查看网络请求
4. **数据库检查**：直接查询数据库确认数据完整性

## 📝 扩展开发

### 添加新的评估器

1. 在 `fastnpc/scripts/init_evaluation_prompts.py` 中添加新的评估器定义
2. 定义评估维度和评分标准
3. 设计提示词模板
4. 运行初始化脚本

### 添加新的测试类型

1. 在 `fastnpc/scripts/generate_test_cases.py` 中添加新的测试模板
2. 实现测试用例生成逻辑
3. 更新UI的测试分类定义
4. 添加相应的后端API

### 自定义评分体系

修改评估提示词模板中的评分标准部分，可以：
- 调整维度权重
- 增加或删除评估维度
- 修改通过标准
- 自定义输出格式

## 🎉 总结

FastNPC 测试与评估系统提供了：
- ✅ 完整的测试用例管理
- ✅ 专业的评估器体系
- ✅ 灵活的配置选项
- ✅ 详细的执行报告
- ✅ 版本化的管理方式

通过系统化的测试和评估，你可以：
- 量化提示词质量
- 对比不同版本效果
- 发现潜在问题
- 持续优化改进

Happy Testing! 🚀

