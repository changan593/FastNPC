# -*- coding: utf-8 -*-
"""
ç”Ÿæˆæç¤ºè¯æµ‹è¯•æŠ¥å‘Š
æ±‡æ€»æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹å’Œè¯„ä¼°ç»“æžœï¼Œç”ŸæˆHTMLæˆ–MarkdownæŠ¥å‘Š
"""
from __future__ import annotations

import sys
import os
from datetime import datetime

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../../..')))

from fastnpc.api.auth.db_utils import _get_conn, _return_conn, _row_to_dict
from fastnpc.config import USE_POSTGRESQL


def generate_markdown_report(output_file: str = "prompt_test_report.md"):
    """ç”ŸæˆMarkdownæ ¼å¼çš„æµ‹è¯•æŠ¥å‘Š"""
    conn = _get_conn()
    try:
        cur = conn.cursor()
        
        # èŽ·å–æ‰€æœ‰æç¤ºè¯
        cur.execute("""
            SELECT id, category, sub_category, name, version, is_active
            FROM prompt_templates
            ORDER BY category, sub_category
        """)
        
        prompts = []
        for row in cur.fetchall():
            if USE_POSTGRESQL:
                prompts.append(_row_to_dict(row, cur))
            else:
                prompts.append(dict(row))
        
        # èŽ·å–æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹
        cur.execute("""
            SELECT id, prompt_category, prompt_sub_category, name, description
            FROM prompt_test_cases
            ORDER BY prompt_category, prompt_sub_category
        """)
        
        test_cases = []
        for row in cur.fetchall():
            if USE_POSTGRESQL:
                test_cases.append(_row_to_dict(row, cur))
            else:
                test_cases.append(dict(row))
        
        # èŽ·å–è¯„ä¼°ç»Ÿè®¡
        cur.execute("""
            SELECT 
                pt.category,
                pt.sub_category,
                pt.name as prompt_name,
                COUNT(pe.id) as eval_count,
                AVG(CAST(pe.manual_score AS FLOAT)) as avg_score
            FROM prompt_templates pt
            LEFT JOIN prompt_evaluations pe ON pt.id = pe.prompt_template_id
            WHERE pt.is_active = 1
            GROUP BY pt.id, pt.category, pt.sub_category, pt.name
            ORDER BY pt.category, pt.sub_category
        """)
        
        eval_stats = []
        for row in cur.fetchall():
            if USE_POSTGRESQL:
                eval_stats.append(_row_to_dict(row, cur))
            else:
                eval_stats.append(dict(row))
        
        # ç”ŸæˆMarkdown
        lines = []
        lines.append("# æç¤ºè¯æµ‹è¯•æŠ¥å‘Š")
        lines.append(f"\nç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        lines.append("\n---\n")
        
        # æ¦‚è§ˆ
        lines.append("## ðŸ“Š æ¦‚è§ˆ")
        lines.append(f"\n- **æç¤ºè¯æ€»æ•°**: {len(prompts)}")
        lines.append(f"- **æ¿€æ´»æç¤ºè¯æ•°**: {len([p for p in prompts if p['is_active']])}")
        lines.append(f"- **æµ‹è¯•ç”¨ä¾‹æ€»æ•°**: {len(test_cases)}")
        
        # æŒ‰ç±»åˆ«ç»Ÿè®¡
        categories = {}
        for prompt in prompts:
            cat = prompt['category']
            if cat not in categories:
                categories[cat] = {'total': 0, 'active': 0}
            categories[cat]['total'] += 1
            if prompt['is_active']:
                categories[cat]['active'] += 1
        
        lines.append("\n### æŒ‰ç±»åˆ«ç»Ÿè®¡\n")
        lines.append("| ç±»åˆ« | æç¤ºè¯æ€»æ•° | æ¿€æ´»æ•° |")
        lines.append("|------|-----------|--------|")
        for cat, stats in sorted(categories.items()):
            lines.append(f"| {cat} | {stats['total']} | {stats['active']} |")
        
        # æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨
        lines.append("\n---\n")
        lines.append("## ðŸ“ æµ‹è¯•ç”¨ä¾‹")
        
        test_by_category = {}
        for tc in test_cases:
            cat = tc['prompt_category']
            if cat not in test_by_category:
                test_by_category[cat] = []
            test_by_category[cat].append(tc)
        
        for cat in sorted(test_by_category.keys()):
            lines.append(f"\n### {cat}")
            lines.append(f"\næµ‹è¯•ç”¨ä¾‹æ•°: {len(test_by_category[cat])}\n")
            
            for tc in test_by_category[cat]:
                sub_cat = tc.get('prompt_sub_category', '')
                lines.append(f"#### {tc['name']}")
                if sub_cat:
                    lines.append(f"**å­åˆ†ç±»**: {sub_cat}")
                lines.append(f"**æè¿°**: {tc['description']}")
                lines.append("")
        
        # è¯„ä¼°ç»“æžœ
        if eval_stats:
            lines.append("\n---\n")
            lines.append("## ðŸ“ˆ è¯„ä¼°ç»“æžœ")
            lines.append("\n| æç¤ºè¯ | ç±»åˆ« | è¯„ä¼°æ¬¡æ•° | å¹³å‡åˆ† |")
            lines.append("|--------|------|----------|--------|")
            
            for stat in eval_stats:
                if stat['eval_count'] > 0:
                    avg_score = stat['avg_score'] if stat['avg_score'] else 'N/A'
                    if isinstance(avg_score, (int, float)):
                        avg_score = f"{avg_score:.2f}"
                    
                    sub_cat = stat.get('sub_category', '')
                    cat_display = f"{stat['category']}" + (f" / {sub_cat}" if sub_cat else "")
                    
                    lines.append(f"| {stat['prompt_name']} | {cat_display} | {stat['eval_count']} | {avg_score} |")
        
        # å†™å…¥æ–‡ä»¶
        content = '\n'.join(lines)
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print(f"âœ“ æµ‹è¯•æŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")
        print(f"  æç¤ºè¯æ€»æ•°: {len(prompts)}")
        print(f"  æµ‹è¯•ç”¨ä¾‹æ€»æ•°: {len(test_cases)}")
        
    finally:
        _return_conn(conn)


def main():
    """ä¸»å‡½æ•°"""
    output_file = sys.argv[1] if len(sys.argv) > 1 else "prompt_test_report.md"
    generate_markdown_report(output_file)
    return 0


if __name__ == "__main__":
    sys.exit(main())

